{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PretrainedEmbeddings_+_Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "PW7xVetszilu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCY268fsID1o"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import regularizers, optimizers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, LSTM, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "import spacy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQmWNRrkKHbg"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights & Biases\n",
        "\n",
        "W&B is a tool that allows to track on website the logs from training of our model. You can login using your GitHub acccount"
      ],
      "metadata": {
        "id": "FzywoeHYzmQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "DWnCojeaoSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "We use data from task [Assess Student Writing Level](https://github.com/jnels13/Screening-Childrens-Writing-Level-With-NLP)."
      ],
      "metadata": {
        "id": "zo2UZkwsz70R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlhAEhJhIHY4"
      },
      "source": [
        "!wget https://github.com/Violet-Spiral/assessing-childrens-writing/raw/main/data/samples_no_title.csv\n",
        " \n",
        "text = pd.read_csv('samples_no_title.csv').dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "F5h8JpoFo9pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[\"Grade\"].unique()"
      ],
      "metadata": {
        "id": "1TPee1pwpuyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuFUmsKiKvhS"
      },
      "source": [
        "text.iloc[23].Text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.iloc[23].Grade"
      ],
      "metadata": {
        "id": "eH3ibAoYpDtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "We define vectorization to create vocabulary and give every token (word) a number."
      ],
      "metadata": {
        "id": "WlcD9wWf0Wj6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM_N4GUsKYdc"
      },
      "source": [
        "nlp = en_core_web_lg.load()\n",
        "Vectorizer = TextVectorization()\n",
        "\n",
        "Vectorizer.adapt(text.Text.to_numpy())\n",
        "vocab = Vectorizer.get_vocabulary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building model\n",
        "\n",
        "Here we define first layer - embeddings"
      ],
      "metadata": {
        "id": "ZBFvXlw10i-z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmlEZdhsKgR5"
      },
      "source": [
        "num_tokens = len(vocab)\n",
        "embedding_dim = len(nlp('The').vector)\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_matrix[i] = nlp(str(word)).vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGkBx_-IKjS5"
      },
      "source": [
        "Embedding_layer=Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define learning rate and epochs parameters (you should change it later and see how it affects training)"
      ],
      "metadata": {
        "id": "BM9JiMcB0sP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = .01\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "7hAZERDsr0KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We put model together"
      ],
      "metadata": {
        "id": "kodleaA304d9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu7hY6XxKlEI"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(Vectorizer)\n",
        "model.add(Embedding_layer)\n",
        "model.add(LSTM(25, return_sequences=True))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='tanh', \n",
        "                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='tanh', \n",
        "                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))    \n",
        "model.add(Dense(1))\n",
        "\n",
        "adam = optimizers.Adam(learning_rate=lr, decay=1e-2)\n",
        "model.compile(optimizer = adam, loss = 'mean_absolute_error', metrics = [\"mean_squared_error\"])\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must initialize W&B - to be easily read on the website, and inform about parameters of training."
      ],
      "metadata": {
        "id": "nNTLDrur07a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "project=\"EmbeddingLayer\", \n",
        "name=f\"with_fixed_embeddings\", \n",
        "config={\n",
        "  \"learning_rate\": lr,\n",
        "  \"architecture\": \"MLP\",\n",
        "  \"dataset\": \"Children texts\",\n",
        "  \"epochs\": epochs})\n",
        "\n",
        "config = wandb.config\n",
        "logging_callback = WandbCallback(log_evaluation=True)"
      ],
      "metadata": {
        "id": "A_bGzgmAoe_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "We don't define particular validation set, indstead, we define validation spit-- 20% of data will be used for validation. \n",
        "\n",
        "We run the training:"
      ],
      "metadata": {
        "id": "2VRZniuV1N5a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "621TLzEfKnS3"
      },
      "source": [
        "history = model.fit(text.Text,\n",
        "          text.Grade,\n",
        "          batch_size = 10,\n",
        "          epochs = epochs,\n",
        "          validation_split=.2,\n",
        "          callbacks=[logging_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the learnig curve, check if it's similar on your W&B site"
      ],
      "metadata": {
        "id": "bZfzd8d21u9j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW8-4BwfN7NJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w_6vuxROAXy"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plot_graphs(history, 'loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMldlQ4jQks5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}