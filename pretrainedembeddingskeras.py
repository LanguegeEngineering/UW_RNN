# -*- coding: utf-8 -*-
"""PretrainedEmbeddingsKeras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLS8RfnQYi5co19K6SheaUN-JYkUoBnH
"""

import pandas as pd
import numpy as np
from tensorflow.keras import regularizers, optimizers
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, LSTM, GlobalMaxPool1D
from tensorflow.keras.models import Sequential
from tensorflow.keras.initializers import Constant
import tensorflow as tf
import spacy

!pip install wandb -qqq
import wandb
wandb.login()
from wandb.keras import WandbCallback

# download and import the large english model.
!python -m spacy download en_core_web_lg
import en_core_web_lg

!wget https://github.com/Violet-Spiral/assessing-childrens-writing/raw/main/data/samples_no_title.csv
 
text = pd.read_csv('samples_no_title.csv').dropna()

len(text)

text["Grade"].unique()

text.iloc[23].Text

text.iloc[23].Grade

nlp = en_core_web_lg.load()
Vectorizer = TextVectorization()

Vectorizer.adapt(text.Text.to_numpy())
vocab = Vectorizer.get_vocabulary()

num_tokens = len(vocab)
embedding_dim = len(nlp('The').vector)
embedding_matrix = np.zeros((num_tokens, embedding_dim))
for i, word in enumerate(vocab):
    embedding_matrix[i] = nlp(str(word)).vector

Embedding_layer=Embedding(
    num_tokens,
    embedding_dim,
    # embeddings_initializer=Constant(embedding_matrix),
    # trainable=False
    )

lr = .01
epochs = 10

model = Sequential()
model.add(Input(shape=(1,), dtype=tf.string))
model.add(Vectorizer)
model.add(Embedding_layer)
model.add(LSTM(25, return_sequences=True))
model.add(GlobalMaxPool1D())
model.add(Dropout(0.5))
model.add(Dense(32, activation='tanh', 
                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))
model.add(Dropout(0.5))
model.add(Dense(32, activation='tanh', 
                kernel_regularizer = regularizers.l1_l2(l1=1e-5, l2=1e-4)))    
model.add(Dense(1))

model.compile(optimizer = tf.keras.optimizers.legacy.Adam(lr, decay=1e-2), loss = 'mean_absolute_error', metrics = ["mean_squared_error"])

print(model.summary())

wandb.init(
project="EmbeddingLayer", 
name=f"with_fixed_embeddings_lr_{lr}", 
config={
  "learning_rate": lr,
  "architecture": "MLP",
  "dataset": "Children texts",
  "epochs": epochs})

config = wandb.config
logging_callback = WandbCallback(log_evaluation=True)

history = model.fit(text.Text,
          text.Grade,
          batch_size = 10,
          epochs = epochs,
          validation_split=.2,
          callbacks=[logging_callback])

import matplotlib.pyplot as plt


def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

plt.figure(figsize=(16, 8))
plot_graphs(history, 'loss')